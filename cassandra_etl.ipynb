{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import Python packages "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import Python packages \r\n",
    "import pandas as pd\r\n",
    "import cassandra\r\n",
    "import os\r\n",
    "import glob\r\n",
    "import csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Check current working directory\r\n",
    "print(os.getcwd())\r\n",
    "\r\n",
    "# Get current folder and subfolder event data\r\n",
    "filepath = os.getcwd() + '/event_data'\r\n",
    "\r\n",
    "# Create a list of files and collect each filepath\r\n",
    "for root, dirs, files in os.walk(filepath):\r\n",
    "    # join the file path and roots with the subdirectories using glob\r\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\jacob\\Documents\\data_engineering_nanodegree\\sparkify-nosql\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Initiating an empty list of rows that will be generated from each file\r\n",
    "full_data_rows_list = [] \r\n",
    "    \r\n",
    "# For every filepath in the file path list \r\n",
    "for f in file_path_list:\r\n",
    "    # Read csv file \r\n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \r\n",
    "        # Create a csv reader object \r\n",
    "        csvreader = csv.reader(csvfile) \r\n",
    "        next(csvreader) # Skip header row\r\n",
    "        # Extract each data row append it to full_data_rows_list        \r\n",
    "        for line in csvreader:\r\n",
    "            full_data_rows_list.append(line) \r\n",
    "            \r\n",
    "print(len(full_data_rows_list)) # Print # of rows\r\n",
    "\r\n",
    "# Create a smaller event data csv file called event_datafile_new.csv that will be used to insert data into the\r\n",
    "# Apache Cassandra tables\r\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\r\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\r\n",
    "    writer = csv.writer(f, dialect='myDialect')\r\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\r\n",
    "                'level','location','sessionId','song','userId'])\r\n",
    "    for row in full_data_rows_list:\r\n",
    "        if (row[0] == ''):\r\n",
    "            continue\r\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8056\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Check the number of rows in the new csv file\r\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\r\n",
    "    print(sum(1 for line in f))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6821\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \r\n",
    "\r\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \r\n",
    "- artist \r\n",
    "- firstName of user\r\n",
    "- gender of user\r\n",
    "- item number in session\r\n",
    "- last name of user\r\n",
    "- length of the song\r\n",
    "- level (paid or free song)\r\n",
    "- location of the user\r\n",
    "- sessionId\r\n",
    "- song title\r\n",
    "- userId\r\n",
    "\r\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\r\n",
    "\r\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating a Cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \r\n",
    "# (127.0.0.1)\r\n",
    "\r\n",
    "from cassandra.cluster import Cluster\r\n",
    "cluster = Cluster()\r\n",
    "\r\n",
    "# To establish connection and begin executing queries, need a session\r\n",
    "session = cluster.connect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create Keyspace"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Create a Keyspace \r\n",
    "try:\r\n",
    "    session.execute(\"\"\"\r\n",
    "    CREATE KEYSPACE IF NOT EXISTS sparkifynosql \r\n",
    "    WITH REPLICATION = \r\n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\r\n",
    ")\r\n",
    "except Exception as e:\r\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set Keyspace"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Set KEYSPACE to the keyspace specified above\r\n",
    "try:\r\n",
    "    session.set_keyspace('sparkifynosql')\r\n",
    "except Exception as e:\r\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Query 1\r\n",
    "\r\n",
    "**Query**: Give me the artist, song title and song's length in the music app history that was heard during sessionId = 338, and itemInSession = 4.\r\n",
    "\r\n",
    "**Table design description**: Based on this query description, the end user needs to be able to filter on sessionId and itemInSession, and retrieve a few other columns. As a result, pairing the two (i.e., sessionId and itemInSession) as the primary key makes sense as it creates a unique identifier and allows the end user to use the two values in the WHERE clause."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "## Query 1: Give me the artist, song title and song's length in the music app history that was heard during\r\n",
    "## sessionId = 338, and itemInSession = 4\r\n",
    "query = \"CREATE TABLE IF NOT EXISTS sparkify_session_item_search \"\r\n",
    "query = query + \"(artist text, song text, length decimal, sessionId int, itemInSession int, PRIMARY KEY (sessionId, itemInSession))\"\r\n",
    "try:\r\n",
    "    session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "## Loop through the data file and insert the rows to the table\r\n",
    "file = 'event_datafile_new.csv'\r\n",
    "\r\n",
    "with open(file, encoding = 'utf8') as f:\r\n",
    "    csvreader = csv.reader(f)\r\n",
    "    next(csvreader) # skip header\r\n",
    "    for line in csvreader:\r\n",
    "        query = \"INSERT INTO sparkify_session_item_search (artist, song, length, sessionId, itemInSession) \"\r\n",
    "        query = query + \"VALUES (%s, %s, %s, %s, %s)\"\r\n",
    "        session.execute(query, (line[0], line[9], float(line[5]), int(line[8]), int(line[3])))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "## Verify the data was entered into the table with a SELECT statement\r\n",
    "query = \"SELECT artist, song, length FROM sparkify_session_item_search WHERE sessionId=338 AND itemInSession=4\"\r\n",
    "try:\r\n",
    "    rows = session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)\r\n",
    "    \r\n",
    "for row in rows:\r\n",
    "    print(row.artist, row.song, row.length)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Faithless Music Matters (Mark Knight Dub) 495.3073\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Query 2\r\n",
    "\r\n",
    "**Query**: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182.\r\n",
    "\r\n",
    "**Table design description**: Based on this query description, the end user needs to be able to filter on sessionId and userId, retrieve a few other columns, and order the results in a particular manner. To be able to filter on userId and sessionId, those were the columns I decided to start with to use as the primary key. When glancing at the data, though, it's easy to see that these two columns alone don't provide a unqique identifier. On top of that, we need to make sure to sort by itemInSession. Because clustering columns sort in the order they are added to the primary key, adding itemInSession to the end of the primary key makes the primary key both unique and sort in the correct order."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "## Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\r\n",
    "## for userid = 10, sessionid = 182\r\n",
    "query = \"CREATE TABLE IF NOT EXISTS sparkify_user_session_search \"\r\n",
    "query = query + \"(artist text, song text, sessionId int, itemInSession int, userId int, firstName text, lastName text, PRIMARY KEY (sessionId, userId, itemInSession))\"\r\n",
    "try:\r\n",
    "    session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "## Loop through the data file and insert the rows to the table\r\n",
    "file = 'event_datafile_new.csv'\r\n",
    "\r\n",
    "with open(file, encoding = 'utf8') as f:\r\n",
    "    csvreader = csv.reader(f)\r\n",
    "    next(csvreader) # skip header\r\n",
    "    for line in csvreader:\r\n",
    "        query = \"INSERT INTO sparkify_user_session_search (artist, song, sessionId, itemInSession, userId, firstName, lastName) \"\r\n",
    "        query = query + \"VALUES (%s, %s, %s, %s, %s, %s, %s)\"\r\n",
    "        session.execute(query, (line[0], line[9], int(line[8]), int(line[3]), int(line[10]), line[1], line[4]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "## Verify the data was entered into the table with a SELECT statement\r\n",
    "query = \"SELECT artist, song, firstName, lastName FROM sparkify_user_session_search WHERE sessionId=182 AND userId=10\"\r\n",
    "try:\r\n",
    "    rows = session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)\r\n",
    "    \r\n",
    "for row in rows:\r\n",
    "    print(row.artist, row.song, row.firstname, row.lastname)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Down To The Bone Keep On Keepin' On Sylvie Cruz\n",
      "Three Drives Greece 2000 Sylvie Cruz\n",
      "Sebastien Tellier Kilometer Sylvie Cruz\n",
      "Lonnie Gordon Catch You Baby (Steve Pitron & Max Sanna Radio Edit) Sylvie Cruz\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Query 3\r\n",
    "\r\n",
    "**Query**: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'.\r\n",
    "\r\n",
    "**Table design description**: Based on this query description, the end user needs to be able to filter on the title of the song, and retrieve the user information. The song title alone, however, is not unique and shouldn't be used in isolation as a primary key. To make it unique, then, adding the userId as a clustering column makes the primary key unique and allows the end user to search for specific songs.\r\n",
    "\r\n",
    "*As a side note, I thought about adding in a different or additional clustering column to make the primary key unique because I could see a world where a userId and song title were not unique on their own. After all, I listen to the same song more than once in different sessions. However, in this dataset, the two made a unique pair, so I decided to leave it for simplicity.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "## Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\r\n",
    "query = \"CREATE TABLE IF NOT EXISTS sparkify_song_search \"\r\n",
    "query = query + \"(firstName text, lastName text, song text, userId int, PRIMARY KEY (song, userId))\"\r\n",
    "try:\r\n",
    "    session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "## Loop through the data file and insert the rows to the table\r\n",
    "file = 'event_datafile_new.csv'\r\n",
    "\r\n",
    "with open(file, encoding = 'utf8') as f:\r\n",
    "    csvreader = csv.reader(f)\r\n",
    "    next(csvreader) # skip header\r\n",
    "    for line in csvreader:\r\n",
    "        query = \"INSERT INTO sparkify_song_search (firstName, lastName, song, userId) \"\r\n",
    "        query = query + \"VALUES (%s, %s, %s, %s)\"\r\n",
    "        session.execute(query, (line[1], line[4], line[9], int(line[10])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "## Verify the data was entered into the table with a SELECT statement\r\n",
    "query = \"SELECT firstName, lastName FROM sparkify_song_search WHERE song='All Hands Against His Own'\"\r\n",
    "try:\r\n",
    "    rows = session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)\r\n",
    "    \r\n",
    "for row in rows:\r\n",
    "    print(row.firstname, row.lastname)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jacqueline Lynch\n",
      "Tegan Levine\n",
      "Sara Johnson\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop the tables before closing out the sessions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "## Drop the tables before closing out the sessions\r\n",
    "query = \"DROP TABLE IF EXISTS sparkify_session_item_search\"\r\n",
    "try:\r\n",
    "    rows = session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)\r\n",
    "\r\n",
    "query = \"DROP TABLE IF EXISTS sparkify_user_session_search\"\r\n",
    "try:\r\n",
    "    rows = session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)\r\n",
    "\r\n",
    "query = \"DROP TABLE IF EXISTS sparkify_song_search\"\r\n",
    "try:\r\n",
    "    rows = session.execute(query)\r\n",
    "except Exception as e:\r\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "session.shutdown()\r\n",
    "cluster.shutdown()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "14670086564b1e26c4aa5cbd349d28a98eafc6d42154a657319680a8b8b15382"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}